{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is modified from the following tutorials on Natural Language Processing:\n",
    "\n",
    "https://medium.com/analytics-vidhya/pythons-natural-language-tool-kit-nltk-tutorial-part-1-645688219a91\n",
    "https://medium.com/@ishan.cdixit/pythons-natural-language-tool-kit-nltk-tutorial-part-2-f5a4d70fd01e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arrow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\arrow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\arrow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arrow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\arrow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\arrow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\arrow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\arrow\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['An an valley indeed so no wonder future nature vanity.', 'Debating all she mistaken indulged believed provided declared.', 'He many kept on draw lain song as same.', 'Whether at dearest certain spirits is entered in to.', 'Rich fine bred real use too many good.', 'She compliment unaffected expression favorable any.', 'Unknown chiefly showing to conduct no.']\n",
      "['An', 'an', 'valley', 'indeed', 'so', 'no', 'wonder', 'future', 'nature', 'vanity', '.', 'Debating', 'all', 'she', 'mistaken', 'indulged', 'believed', 'provided', 'declared', '.', 'He', 'many', 'kept', 'on', 'draw', 'lain', 'song', 'as', 'same', '.', 'Whether', 'at', 'dearest', 'certain', 'spirits', 'is', 'entered', 'in', 'to', '.', 'Rich', 'fine', 'bred', 'real', 'use', 'too', 'many', 'good', '.', 'She', 'compliment', 'unaffected', 'expression', 'favorable', 'any', '.', 'Unknown', 'chiefly', 'showing', 'to', 'conduct', 'no', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "#TOKENISATION.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('state_union')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "EXAMPLE_TEXT = \"An an valley indeed so no wonder future nature vanity. Debating all she mistaken indulged believed provided declared. He many kept on draw lain song as same. Whether at dearest certain spirits is entered in to. Rich fine bred real use too many good. She compliment unaffected expression favorable any. Unknown chiefly showing to conduct no.\"\n",
    "\n",
    "tokened_sent = sent_tokenize(EXAMPLE_TEXT)\n",
    "tokened_word = word_tokenize(EXAMPLE_TEXT)\n",
    "\n",
    "print(tokened_sent)\n",
    "print(tokened_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTHERE ARE A LOT MANY STEMMERS AVAILABLE IN IN THE NLTK LIBRARY:\\n1)PorterStemmer\\n2)SnowballStemmer\\n3)LancasterStemmer\\n4)RegexpStemmer\\n5)RSLPStemmer\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STEMMING\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer()\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))\n",
    " \n",
    "'''\n",
    "THERE ARE A LOT MANY STEMMERS AVAILABLE IN IN THE NLTK LIBRARY:\n",
    "1)PorterStemmer\n",
    "2)SnowballStemmer\n",
    "3)LancasterStemmer\n",
    "4)RegexpStemmer\n",
    "5)RSLPStemmer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase\n",
      "play\n",
      "play\n",
      "playing\n",
      "playing\n",
      "playing\n",
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "#LEMMATIZATION\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('increases'))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\"))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"n\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"a\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"r\"))\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.', 'Here', 'write', 'whatever', 'want', '.', 'You', 'also', 'add', 'big', 'text', 'file', 'see', 'technique', 'works']\n"
     ]
    }
   ],
   "source": [
    "#FILTERING ALL THE STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration. Here you can write whatever you want to. You can also add a very big text file and see how this technique works\"\n",
    "\n",
    "#STOP WORDS ARE PARTICULAR TO RESPECTIVE LANGUAGES(english, spanish, french Et cetera)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if w not in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:1\n",
      "n:29\n",
      " :55\n",
      "a:22\n",
      "v:5\n",
      "l:12\n",
      "e:39\n",
      "y:6\n",
      "i:19\n",
      "d:17\n",
      "s:14\n",
      "o:19\n",
      "w:4\n",
      "r:15\n",
      "f:6\n",
      "u:8\n",
      "t:18\n",
      ".:7\n",
      "D:1\n",
      "b:4\n",
      "g:5\n",
      "h:7\n",
      "m:6\n",
      "k:3\n",
      "p:5\n",
      "c:8\n",
      "H:1\n",
      "W:1\n",
      "R:1\n",
      "S:1\n",
      "x:1\n",
      "U:1\n"
     ]
    }
   ],
   "source": [
    "#COUNTING THE FREQUENCY OF THE WORDS USED\n",
    "import nltk\n",
    "EXAMPLE_TEXT = \"An an valley indeed so no wonder future nature vanity. Debating all she mistaken indulged believed provided declared. He many kept on draw lain song as same. Whether at dearest certain spirits is entered in to. Rich fine bred real use too many good. She compliment unaffected expression favourable any. Unknown chiefly showing to conduct no.\"\n",
    "frequency = nltk.FreqDist(EXAMPLE_TEXT) \n",
    "for key,val in frequency.items(): \n",
    "    print (str(key) + ':' + str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonyms ['fast.n.01', 'fast.v.01', 'fast.v.02', 'fast.a.01', 'fast.a.02', 'fast.a.03', 'fast.s.04', 'fast.s.05', 'debauched.s.01', 'flying.s.02', 'fast.s.08', 'firm.s.10', 'fast.s.10', 'fast.r.01', 'fast.r.02']\n",
      "['slow', 'slow', 'slow']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "for syns in wordnet.synsets('fast'):\n",
    "    synonyms.append(syns.name())\n",
    "print (\"synonyms\", synonyms)\n",
    "\n",
    "#FINDING ANTONYMS FROM WORDNETS\n",
    "from nltk.corpus import wordnet\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"fast\"):\n",
    "    for l in syn.lemmas():\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contact@blahblah.com', 'feedback@blah.com']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Please contact us at contact@blahblah.com for further information.\"+\\\n",
    "        \" You can also give feedback at feedback@blah.com\"\n",
    "\n",
    "\n",
    "emails = re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", text)\n",
    "print (emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#PART OF SPEECH TAGGING\n",
    "import nltk\n",
    "from nltk import word_tokenize \n",
    "from nltk.corpus import state_union \n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    " \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAMED ENTITY RECOGNITION\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "def process():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            namedEnt.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "        \n",
    "process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHUNKING\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "\"\"\"\n",
    "<RB.?>* = \"0 or more of any tense of adverb,\" followed by:\n",
    "<VB.?>* = \"0 or more of any tense of verb,\" followed by:\n",
    "<NNP>+ = \"One or more proper nouns,\" followed by\n",
    "<NN>? = \"zero or one singular noun.\"\n",
    "\"\"\"\n",
    "\n",
    "#process_content()\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            \n",
    "            #ITERATING THROUGH ALL THE SUBTREES AND FILTERING ONLY THE CHUNKS\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "        chunked.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\arrow\\OneDrive\\Desktop\\Summer22Work\\Try1.0\\Python\\NLP_Test.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arrow/OneDrive/Desktop/Summer22Work/Try1.0/Python/NLP_Test.ipynb#ch0000012?line=23'>24</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arrow/OneDrive/Desktop/Summer22Work/Try1.0/Python/NLP_Test.ipynb#ch0000012?line=24'>25</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mstr\u001b[39m(e))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/arrow/OneDrive/Desktop/Summer22Work/Try1.0/Python/NLP_Test.ipynb#ch0000012?line=26'>27</a>\u001b[0m process_content()\n",
      "\u001b[1;32mc:\\Users\\arrow\\OneDrive\\Desktop\\Summer22Work\\Try1.0\\Python\\NLP_Test.ipynb Cell 12'\u001b[0m in \u001b[0;36mprocess_content\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arrow/OneDrive/Desktop/Summer22Work/Try1.0/Python/NLP_Test.ipynb#ch0000012?line=20'>21</a>\u001b[0m         chunkParser \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mRegexpParser(chunkGram)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arrow/OneDrive/Desktop/Summer22Work/Try1.0/Python/NLP_Test.ipynb#ch0000012?line=21'>22</a>\u001b[0m         chunked \u001b[39m=\u001b[39m chunkParser\u001b[39m.\u001b[39mparse(tagged)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/arrow/OneDrive/Desktop/Summer22Work/Try1.0/Python/NLP_Test.ipynb#ch0000012?line=22'>23</a>\u001b[0m         chunked\u001b[39m.\u001b[39;49mdraw()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arrow/OneDrive/Desktop/Summer22Work/Try1.0/Python/NLP_Test.ipynb#ch0000012?line=23'>24</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arrow/OneDrive/Desktop/Summer22Work/Try1.0/Python/NLP_Test.ipynb#ch0000012?line=24'>25</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[1;32mc:\\Users\\arrow\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tree\\tree.py:762\u001b[0m, in \u001b[0;36mTree.draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[39mOpen a new window containing a graphical diagram of this tree.\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdraw\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtree\u001b[39;00m \u001b[39mimport\u001b[39;00m draw_trees\n\u001b[1;32m--> 762\u001b[0m draw_trees(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\arrow\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\draw\\tree.py:1008\u001b[0m, in \u001b[0;36mdraw_trees\u001b[1;34m(*trees)\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_trees\u001b[39m(\u001b[39m*\u001b[39mtrees):\n\u001b[0;32m   1002\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[39m    Open a new window containing a graphical diagram of the given\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[39m    trees.\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \n\u001b[0;32m   1006\u001b[0m \u001b[39m    :rtype: None\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1008\u001b[0m     TreeView(\u001b[39m*\u001b[39;49mtrees)\u001b[39m.\u001b[39;49mmainloop()\n\u001b[0;32m   1009\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arrow\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\draw\\tree.py:998\u001b[0m, in \u001b[0;36mTreeView.mainloop\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    996\u001b[0m \u001b[39mif\u001b[39;00m in_idle():\n\u001b[0;32m    997\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 998\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_top\u001b[39m.\u001b[39mmainloop(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\arrow\\AppData\\Local\\Programs\\Python\\Python310\\lib\\tkinter\\__init__.py:1458\u001b[0m, in \u001b[0;36mMisc.mainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmainloop\u001b[39m(\u001b[39mself\u001b[39m, n\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m   1457\u001b[0m     \u001b[39m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1458\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtk\u001b[39m.\u001b[39;49mmainloop(n)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#CHINKING\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "'''\n",
    "REMOVING FROM THE CHINK ONE OR MORE VERBS, PREPOSITIONS, DETERMINERS, OR THE WORD 'to'\n",
    "'''\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            chunked.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13d31c25b17a5df6c49e47e22a9bf07ae059a35e052de646dc5c648e64bb5b23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
